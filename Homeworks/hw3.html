<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Homework 3</title>
    <link href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css" rel="stylesheet">
    <link href="../hw.css" rel="stylesheet">
    <link href="hw3.css" rel="stylesheet">
</head>
<body>
    <div class="stars">
        <div class="star"></div>
        <div class="star"></div>
        <div class="star"></div>
        <div class="star"></div>
        <div class="star"></div>
        <div class="star"></div>
        <div class="star"></div>
        <div class="star"></div>
        <div class="star"></div>
        <div class="star"></div>
        <div class="star"></div>
        <div class="star"></div>
        <div class="star"></div>
        <div class="star"></div>
        <div class="star"></div>
    </div>
    <nav class="navbar navbar-expand-lg custom-navbar">
        <a class="navbar-brand" href="../index.html">
            <img src="../Images/Navigation.jpg" alt="Navigation" style="height: 40px;">
        </a>
        <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
            <span class="navbar-toggler-icon"></span>
        </button>
        <div class="collapse navbar-collapse" id="navbarNav">
            <ul class="navbar-nav">
                <li class="nav-item">
                    <a class="nav-link">Previous Homeworks: </a>
                </li>
                <li class="nav-item">
                    <a class="nav-link" href="https://sites.google.com/view/blogs-statistics-cyber/homework-1">Homework 1</a>
                </li>
                <li class="nav-item">
                    <a class="nav-link" href="hw2.html">Homework 2</a>
                </li>
            </ul>
        </div>
    </nav>
    <div class="container mt-4">
        <div class="card rounded-border-gradient">
            <div class="card-body">
                <div class="header">
                    <h1 class="header-text">Homework 3</h1>
                </div>
            </div>
        </div>
        <div class="content-section mt-4">
            <h2 class="section-title">Theory Part 1</h2>
            <h3 class="main-title">Absolute Deviations and Their Meaning</h3>
            <p>
                When you compute the absolute deviation from a central point <em>c</em>, you're measuring how far each data point <em>x<sub>i</sub></em> is from that point <em>c</em>, but without worrying about the direction (positive or negative differences). The sum of absolute deviations from a point <em>c</em> for a data set <em>x<sub>1</sub>, x<sub>2</sub>, …, x<sub>n</sub></em> is:
            </p>
            <div class="formula">
                <em>S(c) = ∑<sub>i=1</sub><sup>n</sup> |x<sub>i</sub> - c|</em>
            </div>
            <p>
                The question we are addressing is: 
                <span style="color: red;">Why is the median the value of <em>c</em> that minimizes this sum?</span>
            </p>
            <h3 class="main-title">Case 1: <em>c</em> is the median</h3>
            <p>
                Let’s consider the case where <em>c</em> is the median of the data points. For simplicity, assume the data is ordered as follows:
            </p>
            <div class="formula">
                <em>x<sub>1</sub> ≤ x<sub>2</sub> ≤ ⋯ ≤ x<sub>n</sub></em>
            </div>
            <p>
                The median, denoted as <em>m</em>, is:
            </p>
            <div class="formula">
                <em>If <em>n</em> is odd, <em>m = x<sub>(n+1)/2</sub></em></em>
            </div>
            <div class="formula">
                <em>If <em>n</em> is even, <em>m = (x<sub>n/2</sub> + x<sub>(n/2)+1</sub>)/2</em></em>
            </div>
            <h3 class="main-title">Case 2: What happens if we move <em>c</em> away from the median?</h3>
            <p>
                Now imagine you shift <em>c</em> slightly to the right or left of the median.
            </p>
            <ul>
                <li><strong>If <em>c</em> is to the left of the median</strong>, say <em>c < m</em>, the deviations on the right (the majority of points) increase, and the deviations on the left decrease, but the increase on the right outweighs the decrease on the left.</li>
                <li><strong>If <em>c</em> is to the right of the median</strong>, say <em>c > m</em>, the deviations on the left increase, and the deviations on the right decrease, but again, the increase on the left outweighs the decrease on the right.</li>
            </ul>
            <p>
                Thus, in both cases, moving away from the median increases the total sum of absolute deviations.
            </p>
            <h3 class="main-title">Why does this happen?</h3>
            <p>
                Mathematically, the behavior of the absolute value function is key. The absolute value <em>|x - c|</em> changes differently depending on whether <em>c</em> is greater than or less than <em>x</em>. The function has a "kink" at <em>x = c</em>. Unlike a smooth curve, the absolute value function doesn't have a simple slope everywhere; it changes slope at each data point. When you move the point <em>c</em> away from the median, the sum of the slopes will push the total sum higher.
            </p>
            <p>
                To make this more formal, let's consider the derivative of the sum of absolute deviations. The absolute value function <em>|x - c|</em> behaves as follows:
            </p>
            <div class="formula">
                <em>
                    <div>
                        <span>d</span>
                        <span class="fraction">
                            <span class="top">|x<sub>i</sub> - c|</span>
                            <span class="bottom">dc</span>
                        </span>
                        <span>=</span>
                        <span class="cases">
                            <span class="case">-1 if c < x<sub>i</sub></span><br>
                            <span class="case">1 if c > x<sub>i</sub></span>
                        </span>
                    </div>
                </em>
            </div>
            <p>
                Now, when <em>c</em> is exactly at the median, there are an equal number of data points to the left and to the right of <em>c</em>. This symmetry minimizes the sum of absolute deviations because the overall "push" from the points on either side of <em>c</em> balances out. If you move <em>c</em> away from the median, this balance breaks, causing the sum to increase.
            </p>
            <h2 class="section-title">Theory Part 2</h2>
            <h3 class="main-title">All potential definitions of the "center" of a distribution</h3>
            <p>The concept of a location statistic (or central tendency) refers to ways of identifying a single value that represents the center or typical value of a data distribution.</p>
            <p>There are several ways to define the center of a distribution:</p>
            <ol class="colored-numbers">
                <li><strong class="text-green">Arithmetic Mean (or Average)</strong><br>
                    The sum of all values divided by the number of values. This is sensitive to all values and therefore is influenced by extreme outliers.<br>
                    Formula: <em>μ = (1/n) ∑<sub>i=1</sub><sup>n</sup> x<sub>i</sub></em>
                </li>
                <li><strong class="text-green">Median</strong><br>
                    The middle value when data is ordered. It is robust to outliers and represents the 50th percentile.<br>
                    No specific formula as it depends on ordering.
                </li>
                <li><strong class="text-green">Mode</strong><br>
                    The most frequently occurring value in a dataset. It may not be unique and is not always present for continuous data.
                </li>
                <li><strong class="text-green">Geometric Mean</strong><br>
                    The central tendency of multiplicative processes, often used for positive data, where values are multiplied instead of summed.<br>
                    Formula: <em>GM = (∏<sub>i=1</sub><sup>n</sup> x<sub>i</sub>)<sup>1/n</sup></em>
                </li>
                <li><strong class="text-green">Harmonic Mean</strong><br>
                    The reciprocal of the arithmetic mean of reciprocals, often used for rates or ratios.<br>
                    Formula: <em>HM = n / ∑<sub>i=1</sub><sup>n</sup> (1/x<sub>i</sub>)</em>
                </li>
                <li><strong class="text-green">Trimmed Mean</strong><br>
                    The arithmetic mean after removing a fixed percentage of the largest and smallest data points, reducing the impact of outliers.
                </li>
                <li><strong class="text-green">Winsorized Mean</strong><br>
                    Similar to the trimmed mean, but instead of discarding outliers, extreme values are replaced with the closest non-outlying values.
                </li>
                <li><strong class="text-green">Midrange</strong><br>
                    The average of the maximum and minimum values.<br>
                    Formula: <em>Midrange = (max(x) + min(x)) / 2</em>
                </li>
                <li><strong class="text-green">Quantile-Based Means</strong><br>
                    These generalize the idea of the median by taking values at different quantiles (e.g., 25th percentile, 75th percentile). One can create a "center" by balancing different quantiles, such as calculating a weighted average of multiple quantiles.
                </li>
                <li><strong class="text-green">M-Estimators (Robust Measures of Location)</strong><br>
                    Generalizations of the mean that use an iterative process to reduce the influence of outliers (e.g., Huber's M-estimator).
                </li>
                <li><strong class="text-green">L-estimators</strong><br>
                    Linear combinations of order statistics (e.g., averages of specific quantiles) can define alternative "center" values. The midhinge (average of the 25th and 75th percentiles) is an example.
                </li>
                <li><strong class="text-green">Generalized Means (Power Mean)</strong><br>
                    A family of means parametrized by an exponent <em>p</em>. When <em>p = 1</em>, you get the arithmetic mean; when <em>p = 0</em>, you get the geometric mean; when <em>p = -1</em>, you get the harmonic mean.<br>
                    Formula: <em>M<sub>p</sub>(x<sub>1</sub>, x<sub>2</sub>, …, x<sub>n</sub>) = ( (1/n) ∑<sub>i=1</sub><sup>n</sup> x<sub>i</sub><sup>p</sup> )<sup>1/p</sup></em>
                </li>
                <li><strong class="text-green">Median of Means</strong><br>
                    Break the data into equal-sized chunks, compute the mean of each chunk, and take the median of these means. This estimator is robust to outliers.
                </li>
                <li><strong class="text-green">Barycenter in Metric Spaces</strong><br>
                    In more complex spaces (such as probability spaces or geometrically complex spaces), the concept of a central tendency is generalized to a barycenter, the generalization of a mean in non-Euclidean spaces.
                </li>
                <li><strong class="text-green">Entropic Mean</strong><br>
                    Based on information theory, the entropic mean minimizes the relative entropy (or Kullback-Leibler divergence) between a probability distribution and a target point.
                </li>
                <li><strong class="text-green">Mode-based Averages</strong><br>
                    While mode usually refers to the most frequent value, variations can include multiple modes or centers of clusters, creating a definition of central tendency in multi-modal distributions.
                </li>
                <li><strong class="text-green">Minimum Distance Estimators</strong><br>
                    The "center" is defined as the value that minimizes some measure of distance from the data. Examples include the least squares criterion (arithmetic mean minimizes the sum of squared errors) or the least absolute deviations (median minimizes the sum of absolute deviations).
                </li>
                <li><strong class="text-green">Fréchet Mean</strong><br>
                    For data in non-Euclidean spaces, like on a manifold, the Fréchet mean generalizes the concept of a mean to minimize the expected distance between points.
                </li>
                <li><strong class="text-green">Center of Gravity (for Distributions)</strong><br>
                    In physics-based or geometry-inspired approaches, the concept of a "center" is generalized as the center of mass, applying integration for continuous distributions.
                </li>
            </ol>
            <h3 class="main-title">Why can there be potentially infinite definitions of the "center"?</h3>
            <ul>
                <li><strong>Parameterized Families of Means:</strong> By defining functions that depend on parameters, you can create a limitless range of means. </li>
                <li><strong>Distance Function Generalization:</strong> Any function that measures the distance between a dataset and a candidate center can serve as a basis for a location statistic. </li>
                <li><strong>Weighted Means/Quantiles:</strong> Assigning weights to different parts of the distribution can result in infinite generalizations. </li>
                <li><strong>Optimization Frameworks:</strong> Many location statistics arise as solutions to optimization problems (e.g., minimizing a loss function). Since the set of possible loss functions is infinite, this approach provides a pathway to an endless variety of location measures.</li>
                <li><strong>Bayesian Means:</strong> Bayesian approaches to statistics can define a "central tendency" through posterior distributions, which can be specified using different prior beliefs, leading to infinite variations in the resulting mean or mode.</li>
            </ul>
            <p>By continuously altering distance metrics, error functions, weighting schemes, or optimization criteria, we can define an endless variety of central tendency measures.</p>
            

        </div>
        <div class="content-section">
            <h2 class="section-title">Practice</h2>
            <iframe src="../WebAttackSimulation/Pages/Index.html"></iframe>
        </div>
    </div>
    <script src="https://code.jquery.com/jquery-3.5.1.slim.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/@popperjs/core@2.5.4/dist/umd/popper.min.js"></script>
    <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/js/bootstrap.min.js"></script>
    <script>
        window.addEventListener('message', function(event) {
            const iframe = document.querySelector('.content-section iframe');
            if (iframe && Number.isInteger(event.data)) {
                iframe.style.height = event.data + 'px';
            }
        });
        // Function to send messages to the iframe after it loads
        function sendMessagesToIframe() {
            const iframe = document.querySelector('.content-section iframe');
            if (iframe && iframe.contentWindow) {
                // Message to set the "Random Walk (RW)" radio button as checked
                iframe.contentWindow.postMessage({ action: 'setProcess', processId: 'poisson' }, '*');
    
                // Delay clicking the recompute button to ensure the radio button is set
                setTimeout(function() {
                    iframe.contentWindow.postMessage({ action: 'clickRecompute' }, '*');
                }, 500); // Adjust the delay as needed
            }
        }
        // Attach the function to the iframe's load event
        const iframeElement = document.querySelector('.content-section iframe');
        if (iframeElement) {
            iframeElement.onload = sendMessagesToIframe;
        }
    </script>
</body>
</html>